---
layout: post
title: numpy로 CNN 구성하기
subtitle: cs231n 과제 리뷰
date: 2022-03-17
last_modified_at: 2022-03-17
tags: [deep learning, cs231n, CNN, numpy]
---

스탠포드의 cs231n 수업 과제인 ConvolutionalNetworks.ipynb를 풀면서 배운 내용 정리하기 위해 이 포스트를 작성한다.

## convolution
![CNN의 구조](https://editor.analyticsvidhya.com/uploads/90650dnn2.jpeg)
일반적인 CNN 모델의 구조.

### conv forward
reference:
1. [Prof. Andrew Ng 수업](https://www.youtube.com/watch?v=3PyJA9AfwSk)

![Convolution 작동 방식](https://i.stack.imgur.com/QZsRB.png)

합성망 순전파를 개념 그대로 구현하는 코드. 실제로는 파이썬으로 구현하면 매우 비효율적이기 때문에 C를 사용한 다른 로직을 사용한다고 한다.
input의 shape이 (N, C, H, W)이고 필터 웨이트의 그것은 (F, C, FH, FW)이며 stride == S, pad == P일 때, 출력되는 텐서의 모양은 (N, F, OH, OW)이다. 이 때,
<!-- $$ 
OH = 1 + \frac{H + 2 * P - FH}{S} \\
OW = 1 + \frac{W + 2 * P - FW}{S}
$$ --> 

<div align="center"><img style="background: white;" src="..\svg\MLHbiVLC6Z.svg"></div> 
이다.
패딩은 1. 커널의 크기만큼 아웃풋 텐서의 크기가 줄어드는 것을 상쇄하고 2. 가장자리에 위치한 픽셀값들이 과소 반영(반대로 중앙부 픽셀들이 과대 반영)되는 것을 방지하는 역할을 한다.

```python
def conv_forward_naive(x, w, b, conv_param):

    p, s = conv_param['pad'], conv_param['stride']
    N, C, H, W = x.shape # input height/width
    F, _, HH, WW = w.shape # filter height/width
    OH, OW = 1 + (H + 2 * p - HH) // s, 1+ (W + 2 * p - WW) // s # output height/width
    out = np.zeros((N, F, OH, OW))
    padded = np.pad(x, pad_width=((0,0), (0,0), (p,p), (p,p)) )

    for n in range(N):
        for f in range(F):
            for i in range(OH):
                start_i = s * i
                for j in range(OW):
                    start_j = s*j
                    target = padded[n, : , start_i : start_i + HH, start_j : start_j+WW]
                    out[n,f,i,j] = np.sum(target * w[f]) + b[f]

    cache = (x, w, b, conv_param)
    return out, cache
```
넘파이에서는 [np.pad](https://numpy.org/doc/stable/reference/generated/numpy.pad.html)라는 함수를 제공하므로 손쉽게 패딩을 구현할 수 있다. input 텐서의 차원 중 batch, channel차원을 제외하고 가로, 세로 차원에만 패딩을 적용한다.

### conv backward

```python
def conv_backward_naive(dout, cache):
    """
    A naive implementation of the backward pass for a convolutional layer.

    Inputs:
    - dout: Upstream derivatives. (N, F, Output_H, Output_W)
    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive
        - x: Input data of shape (N, C, H, W)
        - w: Filter weights of shape (F, C, HH, WW)
        - b: Biases, of shape (F,)
    Returns a tuple of:
    - dx: Gradient with respect to x
    - dw: Gradient with respect to w
    - db: Gradient with respect to b
    """

    x, w, b, conv_param = cache
    s, p = conv_param['stride'], conv_param['pad']

    dx = np.zeros_like(x)
    dw = np.zeros_like(w)

    _, _, H, W = x.shape
    _, _, HH, WW = w.shape
    N, F, output_H, output_W = dout.shape

    padded = np.pad(x, ((0,0), (0,0), (p,p), (p,p)))
    padded_dx = np.pad(dx, ((0,0), (0,0), (p,p), (p,p)))

    db = np.sum(dout, axis=(0,2,3)) # filter unit별로 bias가 있으므로 filter의 차원은 빼고

    for n in range(N):
        for f in range(F):
            for i in range(output_H):
                start_i = s * i
                for j in range(output_W):
                    start_j = s * j
                    target = padded[n, :, start_i:start_i+HH, start_j:start_j+WW]
                    padded_dx[n, :, start_i:start_i+HH, start_j:start_j+WW] += dout[n, f, i, j] * w[f]
                    dw[f] += dout[n,f,i,j] * target
    dx = padded_dx[:,:, 1:H+1, 1:W+1]

    return dx, dw, db
```
Fully connected layer에서와 같이 로컬 dX, dW, db 는 각각 W, X, 1이다. 다른 점은 dX의 경우 원래 패딩이 없었기에 맨 처음과 맨 마지막 내적값은 제외시켜 줘야 하며(패드가 커널보다 작기 때문에 하나씩만 제외), db의 경우 채널의 수와 상관없이 히든 유닛당 bias가 하나기 때문에 필터 차원의 제외하고 줄이면 된다. (e.g. 인풋이 RGB 세 체널로 이뤄진 경우에도 <!-- $h = W_{red}X_{red} + W_{green}X_{green} + W_{blue}X_{blue} + b$ --> <img style="transform: translateY(0.1em); background: white;" src="..\svg\iNpIFJ2cSd.svg"> ) dW의 경우 하나의 W가 모든 픽셀을 담당하므로 중복되어 더해진다.

## pooling

### max pooling forward

커널에 대응되는 부분 중 가장 큰 값만 추출.

```python
def max_pool_forward_naive(x, pool_param):
    """
    A naive implementation of the forward pass for a max-pooling layer.

    Inputs:
    - x: Input data, of shape (N, C, H, W)
    - pool_param: dictionary with the following keys:
      - 'pool_height': The height of each pooling region
      - 'pool_width': The width of each pooling region
      - 'stride': The distance between adjacent pooling regions

    No padding is necessary here. Output size is given by 

    Returns a tuple of:
    - out: Output data, of shape (N, C, H', W') where H' and W' are given by
      H' = 1 + (H - pool_height) / stride
      W' = 1 + (W - pool_width) / stride
    - cache: (x, pool_param)
    """

    pool_h, pool_w, s = pool_param.values()
    N, C, H, W = x.shape
    output_h, output_w = 1+ (H-pool_h) // s, 1+ (W - pool_w) // s

    out = np.zeros((N, C, output_h, output_w))
    for n in range(N):
        for c in range(C):
            for i in range(output_h):
                start_i = s * i
                for j in range(output_w):
                    start_j = s * j
                    target = x[n,c, start_i : start_i+pool_h, start_j:start_j+pool_w]
                    target_max = target.max()
                    out[n,c,i,j] = target_max

    cache = (x, pool_param)
    return out, cache
```
pooling은 개념이나 코드나 어려운 점이 없다. mean pooling은 위에서 array.mean() 하면 된다.

### max pooling backward

```python
def max_pool_backward_naive(dout, cache):
    """
    A naive implementation of the backward pass for a max-pooling layer.

    Inputs:
    - dout: Upstream derivatives #(N, F, H, W)
    - cache: A tuple of (x, pool_param) as in the forward pass.

    Returns:
    - dx: Gradient with respect to x
    """
    N, F, output_H, output_W = dout.shape
    x, pool_param = cache
    pool_H, pool_W, s = pool_param.values()

    dx = np.zeros_like(x)
    for n in range(N):
        for f in range(F):
            for i in range(output_H):
                start_i = i *s
                for j in range(output_W):
                    start_j = j*s
                    target = x[n,f, start_i:start_i+pool_H, start_j:start_j+pool_W]
                    max_idx = np.max(target)
                    bool_matrix = (target == max_idx)
                    dx[n,f, start_i:start_i+pool_H, start_j:start_j+pool_W] += dout[n,f,i,j] * bool_matrix

    return dx
```

max pool의 경우 풀링이 된 값들만 1을 곱해 업데이트를 하면 된다. mean pooling은 모든 값들에 1/n (n은 풀링된 요소들의 갯수)를 곱하면 된다.

## fully connected layer
일반적으로 합성곱층 이후에는 분류를 위한 FC층과 softmax 층이 더해진다. FC에 인풋으로 전달하기 위해 다차원의 X를 flatten하는 과정이 필요하다. 배치 차원만 제외하고 flatten할 수 있는 파이토치와 달리 numpy.flatten()은 무조건 1차원으로 축소하기 때문에 넘파이에서는 reshape(N, -1) 로 하면 된다.

